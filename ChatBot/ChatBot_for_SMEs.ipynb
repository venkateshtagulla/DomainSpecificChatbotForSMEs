# Install dependencies
!pip install PyMuPDF faiss-gpu transformers pdfplumber sentence-transformers bitsandbytes huggingface_hub
from huggingface_hub import login

# Replace 'your_access_token' with the token you copied from your Hugging Face account
login()


from google.colab import files
import pdfplumber
import faiss
from sentence_transformers import SentenceTransformer
import torch
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

# Upload PDF file
uploaded = files.upload()

# Extract text from the uploaded PDF
def extract_text_from_pdf(pdf_path):
    with pdfplumber.open(pdf_path) as pdf:
        text = ''
        for page in pdf.pages:
            text += page.extract_text()
    return text

# Assuming the uploaded file is the first one
pdf_path = list(uploaded.keys())[0]
document_text = extract_text_from_pdf(pdf_path)
print("Extracted Text (Preview):", document_text[:500])  # Display first 500 characters

# Initialize the embedding model
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

# Split text into chunks
def split_text_into_chunks(text, chunk_size=512):
    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]

chunks = split_text_into_chunks(document_text)

# Generate embeddings for each chunk
embeddings = embedding_model.encode(chunks, convert_to_tensor=False)

# Create and populate the FAISS index
dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(embeddings)

print(f"FAISS index created with {index.ntotal} vectors.")
# Define the quantization configuration
# Quantization for int8
quantization_config = BitsAndBytesConfig(load_in_8bit=True)
READER_MODEL_NAME = "meta-llama/Llama-3.1-8B-Instruct"  # Can be replaced with a larger model if desired

# Load the model with quantization
model = AutoModelForCausalLM.from_pretrained(
    READER_MODEL_NAME,
    torch_dtype=torch.float16,
    quantization_config=quantization_config,
    device_map="cuda:0",
)


tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)

from transformers import pipeline

# Initialize the pipeline for text generation
READER_LLM = pipeline(
    task="text-generation",
    model=model,
    tokenizer=tokenizer,
    do_sample=True,
    temperature=0.5,  # Adjust the temperature for creativity
    repetition_penalty=1.2,  # Penalize repetitive sequences
    max_new_tokens=300,  # Limit the length of generated output
)

PROMPT_TEMPLATE = """
You are a highly intelligent assistant tasked with answering only the specific question asked, based strictly on the provided context. Follow these rules without exception:

1. **Answer the question provided accurately** based only on the given context. Do not include or generate any additional questions or commentary.
2. **Do not provide any extra details, comments, or explanations** beyond what is strictly required to answer the question.
3. If the answer is not found in the context, respond **only** with: "No answer found."
4. Avoid generating new questions or assumptions. Respond only to the question explicitly stated.
5.strictly return answer to the given question from context no need to return context and question simply return correct answer in well structured paragraph.
**Context:**
{context}

**Question:**
{question}
**return  only answer no need of context and question**

**Answer:**

"""


# Function to retrieve relevant chunks
def retrieve_relevant_chunks(query, k=5):
    # Encode the query
    query_embedding = embedding_model.encode([query], convert_to_tensor=False)

    # Retrieve top-k similar chunks
    distances, indices = index.search(query_embedding, k)
    return [chunks[i] for i in indices[0]]
# Initialize a cache to store previously answered questions and their answers
question_answer_cache = {}
def generate_answer_with_cache(query):
    # Check if the query is already in the cache
    if query in question_answer_cache:
        print("Answer retrieved from cache:")
        return question_answer_cache[query]

    # If not in cache, generate a new answer
    print("Processing query for the first time:")
    relevant_chunks = retrieve_relevant_chunks(query)
    context = " ".join(relevant_chunks)

    # Prepare input for the model
    final_prompt = PROMPT_TEMPLATE.format(question=query, context=context)
    response = READER_LLM(final_prompt)
    answer = response[0]["generated_text"]
    answer = answer.split("Answer:")[-1].strip()

    # Store the answer in the cache
    question_answer_cache[query] = answer

    print(answer)
    return answer

----------------------------------------------------------------------------------------------------------------------------------------------------------------------
from flask import Flask, request, jsonify
from flask_cors import CORS
from pyngrok import ngrok
ngrok.set_auth_token("2pncbTQ8innH8iOCAlhRNqmK5Zo_3iVTjEeQkcpVLtgYZWLQM")
# Initialize the Flask app
app = Flask(__name__)
CORS(app, resources={r"/*": {"origins": "*"}})
#CORS(app, resources={r"/*": {"origins": "http://localhost:5173"}})
@app.route('/chat', methods=['POST'])
def chat():
    # Get the query from the frontend
    data = request.get_json()
    query = data.get("message", "")
    try:
        answer = generate_answer_with_cache(query)
        print(f"Generated answer: {answer}")
    except Exception as e:
        return jsonify({"error": str(e)}), 500

    return jsonify({"response": answer})
if __name__ == '__main__':
    # Expose the Flask app using ngrok
    public_url = ngrok.connect(5000)
    print("Public URL:", public_url)
    app.run(port=5000)
